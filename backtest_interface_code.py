"""
====================================================================================
Fichier : backtest_interface_code.py
Objectif : Interface autonome de backtesting d'une strat√©gie de trading param√©tr√©e
====================================================================================

Description g√©n√©rale :
Ce module permet d'ex√©cuter un backtest complet d'une strat√©gie algorithmique sur 
donn√©es historiques, √† partir d'un simple dictionnaire d‚Äôhyperparam√®tres. Il centralise 
la logique d‚Äôinterface entre :

    - le chargement de donn√©es multi-unit√©s de temps,
    - la construction des param√®tres de strat√©gie,
    - l‚Äôappel au moteur de backtest vectoriel,
    - et la journalisation des r√©sultats dans une base SQLite.

Ce fichier sert de brique d‚Äôex√©cution unitaire pour automatiser des s√©ries de tests
(exp√©rimentations GP, AutoML, BO, etc.).

Contexte m√©thodologique :
Le pipeline assume un paradigme bas√© sur des simulations ex-post (backtesting),
o√π chaque combinaison d‚Äôhyperparam√®tres d√©finit une strat√©gie unique. La performance
de cette strat√©gie est mesur√©e √† l‚Äôaide d‚Äôindicateurs robustes comme :

    - Sharpe Ratio (moyenne des rendements / volatilit√© empirique)
    - Max Drawdown (perte maximale relative)
    - Profit Factor (total gains / total pertes)
    - Win Rate (proportion de trades gagnants)

Composants internes appel√©s :
- `scalping_backtest_engine.py` : moteur vectoris√© de backtesting.
- `strategy_params_core.py` : constructeur param√©trique `StrategyParams`.
- `parallel_backtest_launcher.py` : utilitaire de chargement des donn√©es OHLCV multi-timeframe.
- `backtest_db_manager.py` : conversion des logs en `DataFrame` propre.
- `sqlite3` : stockage persistant des r√©sultats dans `trade_logs.db`.

Fonction centrale :
    - `run_backtest_from_params(param_dict: dict) -> bool`
        Cette fonction encapsule le cycle complet :
            ‚Üí pr√©paration des donn√©es,
            ‚Üí g√©n√©ration des param√®tres,
            ‚Üí ex√©cution du backtest,
            ‚Üí calcul et journalisation des KPIs,
            ‚Üí validation finale.

Utilisation typique :
Appel√©e de fa√ßon it√©rative dans une boucle d‚Äôoptimisation (GP, BoTorch, Grid Search),
ou bien int√©gr√©e dans un moteur d‚Äôexploration AutoML pour √©valuer des milliers de
configurations.

Entr√©es :
    - `param_dict` (dict) : hyperparam√®tres de la strat√©gie (seuils, dur√©es, poids, etc.)

Sorties :
    - Bool√©en : succ√®s ou √©chec du backtest (selon la pr√©sence des m√©triques critiques).

S√©curit√© m√©moire :
    - Les donn√©es march√© sont clon√©es √† chaque appel (pas de mutation du cache global)
    - Utilisation de `gc.collect()` syst√©matique en `finally` pour √©viter toute fuite
    - Les UUIDs garantissent la tra√ßabilit√© de chaque ex√©cution dans la base de logs

Cas d‚Äôusage typiques :
    - Boucle AutoML ou Meta-Backtesting
    - Fonction de score dans un framework d‚Äôoptimisation (BoTorch, Optuna, Ax)
    - Interface API pour scoring de configurations RL ou rule-based

Auteur : Moncoucut Brandon
Version : Juin 2025
"""


# === Imports fondamentaux ===
import sqlite3
import uuid
import numpy as np
import pandas as pd  # ‚Üê daily PnL stats
import gc
from typing import Tuple, Dict

# Annualisation daily pour crypto 24/7 (fallback robuste)
try:
    from config import ANN_FACTOR_DAILY
except Exception:
    ANN_FACTOR_DAILY = 365

from scalping_signal_engine import get_pair_info
from scalping_backtest_engine import backtest_strategy
from strategy_params_core import init_params, params_to_dict
from parallel_backtest_launcher import load_or_fetch_market_data

# DB helpers (DDL/migration + insert + KPI upsert)
from backtest_db_manager import (
    array_to_clean_dataframe,
    init_sqlite_database,
    migrate_sqlite_schema,
    insert_logs_from_array,
    insert_trades_from_array,
    upsert_kpis,
)

# Calcul des KPI ‚Äúconsistency-first‚Äù
import metrics_core as mx
# Annualisation & garde-fous (crypto 24/7) ‚Äî fallback robuste si config indisponible
try:
    from config import ANN_FACTOR_DAILY, SHARPE_CAP, MIN_ACTIVE_DAYS, MIN_STD_DAILY
except Exception:
    ANN_FACTOR_DAILY = 365
    SHARPE_CAP = 5.0
    MIN_ACTIVE_DAYS = 30
    MIN_STD_DAILY = 1e-6


# üîÑ Fichier autonome pour ex√©cuter un backtest √† partir d'un dictionnaire de param√®tres

PAIR = "BTCUSDC"
NB_DAYS = 185
INTERVALS = ["1m", "5m", "15m", "1h"]
DATA_PATH = "/Users/brandonmoncoucut/Desktop/Najas_king/log_sqlite/backtest_pipeline/histo_data_185d.db"
FINAL_DB_PATH = "/Users/brandonmoncoucut/Desktop/Najas_king/log_sqlite/backtest_pipeline/trade_logs.db"

INIT_BALANCE = 5000.0
FEE_RATE = 0.001
RISK_PER_TRADE = 0.02  # 2% (fraction)

# Cache des donn√©es (une seule fois pour toute la session)
_market_data_cache = None

def run_backtest_from_params(param_dict: dict, iteration: int = 0) -> Tuple[bool, str, Dict[str, float]]:
    """
    Ex√©cute un backtest unitaire √† partir d'un jeu d'hyperparam√®tres, persiste les donn√©es
    dans SQLite (init + migration idempotentes), calcule les KPI via metrics_core et fait
    un UPSERT des KPI.

    Args:
        param_dict (dict): Hyperparam√®tres de la strat√©gie (r√©els ou normalis√©s).
        iteration (int): Identifiant d'it√©ration/batch (par d√©faut 0) utilis√© comme PK avec backtest_id.

    Returns:
        Tuple[bool, str, Dict[str, float]]:
            - success (bool): True si KPI cl√©s valides et upsert√©s, False sinon.
            - backtest_id (str): UUID de l'ex√©cution pour tra√ßabilit√©.
            - kpi (dict): Dictionnaire des KPI calcul√©s (peut contenir des NaN si donn√©es manquantes).

    Notes
    -----
    - Les inserts utilisent les helpers d√©di√©s (array‚ÜíDB) pour pr√©server les dtypes.
    - L'UPSERT KPI s'appuie sur la cl√© primaire (backtest_id, iteration).
    """

    global _market_data_cache

    try:
        # Chargement ou mise en cache des donn√©es march√© (multi-timeframe)
        if _market_data_cache is None:
            _market_data_cache = load_or_fetch_market_data(PAIR, INTERVALS, NB_DAYS, DATA_PATH)

        # R√©cup√©ration des contraintes de trading pour la paire (quantit√©s et prix)
        min_qty, step_size, tick_size = get_pair_info(PAIR)

        # G√©n√©ration des param√®tres sous forme d'objet StrategyParams (avec injection BoTorch)
        params = init_params(min_qty, step_size, tick_size, custom_values=param_dict)

        # ID unique pour tracer ce backtest
        backtest_id = str(uuid.uuid4())

        # Duplication des donn√©es pour ne pas muter le cache global
        data_clone = {
            interval: {col: arr.copy() for col, arr in interval_data.items()}
            for interval, interval_data in _market_data_cache.items()
        }

        # ===== 1) Lancement du backtest (moteur vectoriel) =====
        trade_history, log_array, early_stop = backtest_strategy(
            data_clone,
            INIT_BALANCE,
            params,
            FEE_RATE,
            RISK_PER_TRADE
        )

        # ===== 2) Init + migration du sch√©ma DB (idempotent) =====
        init_sqlite_database(FINAL_DB_PATH)
        migrate_sqlite_schema(FINAL_DB_PATH)

        # ===== 3) Inserts (logs & trades) ‚Äî robustes via helpers dtype-aware =====
        _ = insert_logs_from_array(FINAL_DB_PATH, backtest_id, iteration, log_array)

        # ===== 4) KPI ‚Äúconsistency-first‚Äù =====
        df_log = array_to_clean_dataframe(log_array)
        df_trade = array_to_clean_dataframe(trade_history)

        # NB: compute_intraday_consistency_kpis tol√®re des colonnes manquantes (remplies √† NaN)
        #     On tente ann_factor=ANN_FACTOR_DAILY, sinon on retombe sur signature historique.
        try:
            kpi_raw = mx.compute_intraday_consistency_kpis(
                df_trades=df_trade,
                df_logs=df_log,
                price_col="price",
                timestamp_col="timestamp",
                fee_rate=FEE_RATE,
                ann_factor=ANN_FACTOR_DAILY,  # ‚Üê peut ne pas exister selon ta version
            )
        except TypeError:
            kpi_raw = mx.compute_intraday_consistency_kpis(
                df_trades=df_trade,
                df_logs=df_log,
                price_col="price",
                timestamp_col="timestamp",
                fee_rate=FEE_RATE,
            )

        # --- Enrichissement KPI utile √† la s√©lection ---
        kpi = dict(kpi_raw)  # shallow copy

        # 1) Harmonisation Sharpe 365 :
        #    - si la lib renvoie d√©j√† 'sharpe_d_365', on garde
        #    - sinon si 'sharpe_daily_ann' ‚Üí mappe dessus
        #    - sinon si 'sharpe_d_252' ‚Üí convertit vers 365 via ‚àö(365/252)
        if "sharpe_d_365" not in kpi or kpi.get("sharpe_d_365") is None:
            if "sharpe_daily_ann" in kpi and kpi["sharpe_daily_ann"] is not None:
                kpi["sharpe_d_365"] = float(kpi["sharpe_daily_ann"])
            elif "sharpe_d_252" in kpi and kpi["sharpe_d_252"] is not None:
                try:
                    kpi["sharpe_d_365"] = float(kpi["sharpe_d_252"]) * float(np.sqrt(365.0 / 252.0))
                except Exception:
                    pass  # on laisse None si conversion impossible

        # 2) % jours verts : alias si besoin (pct_green_days est ce que consomme l‚Äôaval)
        if "pct_green_days" not in kpi and "green_days_ratio" in kpi and kpi["green_days_ratio"] is not None:
            kpi["pct_green_days"] = float(kpi["green_days_ratio"])

        # 3) mdd_abs (copie sign-free de max_drawdown)
        if "mdd_abs" not in kpi and "max_drawdown" in kpi and kpi["max_drawdown"] is not None:
            try:
                kpi["mdd_abs"] = float(abs(kpi["max_drawdown"]))
            except Exception:
                kpi["mdd_abs"] = None

        # 4) n_unique_days (fallback l√©ger sans pandas)
        if "n_unique_days" not in kpi or kpi.get("n_unique_days") is None:
            try:
                if df_trade is not None and not df_trade.empty and "timestamp" in df_trade.columns:
                    ts = df_trade["timestamp"].to_numpy()
                    ts = ts[~np.isnan(ts)]
                    if ts.size > 0:
                        # Heuristique ms vs s
                        denom = 86_400_000.0 if float(np.nanmax(ts)) > 1e12 else 86_400.0
                        days_bucket = (ts // denom).astype(np.int64)
                        kpi["n_unique_days"] = int(np.unique(days_bucket).size)
                    else:
                        kpi["n_unique_days"] = None
                else:
                    kpi["n_unique_days"] = None
            except Exception:
                kpi["n_unique_days"] = None

        # (ulcer_index, top5_share, profit_factor sont propag√©s tels quels si fournis par metrics_core)
        # ----------------------------------------------------------------

        # UPSERT KPI dans la table d√©di√©e
        upsert_kpis(FINAL_DB_PATH, backtest_id, iteration, kpi)

        def _ensure_sharpe_365(k: dict, ann_factor: int) -> dict:
            """
            Garantit une cl√© 'sharpe_d_365' dans le dict KPI, avec logique ‚Äúdefense in depth‚Äù.
            Ordre des tentatives :
            1) Si 'sharpe_d_365' existe ‚Üí inchang√©.
            2) Si 'sharpe_daily_ann' existe (nom g√©n√©rique) ‚Üí remap vers 'sharpe_d_365'.
            3) Sinon, si 'mean_daily_return' & 'vol_daily_return' dispo ‚Üí calcule Sharpe = (m/s)*sqrt(ann_factor).
            4) Sinon, fallback: laisse 'sharpe_d_365' manquant (None) plut√¥t que d‚Äôinventer.
            NB: Pas de conversion na√Øve depuis 'sharpe_d_252' ‚Üí √©vite des erreurs d‚Äô√©chelle.
            """
            out = dict(k) if isinstance(k, dict) else {}
            if out.get("sharpe_d_365") is not None:
                return out
            if out.get("sharpe_daily_ann") is not None:
                out["sharpe_d_365"] = float(out["sharpe_daily_ann"])
                return out
            m, s = out.get("mean_daily_return"), out.get("vol_daily_return")
            if (m is not None) and (s is not None) and (s not in (0, 0.0)):
                try:
                    out["sharpe_d_365"] = float(m) / float(s) * (ann_factor ** 0.5)
                    return out
                except Exception:
                    pass
            out.setdefault("sharpe_d_365", None)
            return out

        # Normalisation des noms ‚Üí DB standardis√©e
        kpi = _ensure_sharpe_365(kpi_raw, ANN_FACTOR_DAILY)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        #  Garde-fous ‚Äúquant-grade‚Äù : robustifier et filtrer les outliers √† la source
        #    - n_unique_days : nb. de jours actifs de PnL (qualit√© d'√©chantillon)
        #    - std_daily     : √©cart-type des PnL journaliers (√©vite Sharpe instable)
        #    - flag_sharpe_outlier / is_valid / invalid_reason
        #  NB: on taggue en DB et on **hard-reject** (is_valid=0) pour ne rien polluer c√¥t√© GP.
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        def _safe_int(x):
            try:
                return int(x) if x is not None and np.isfinite(x) else None
            except Exception:
                return None

        def _safe_float(x):
            try:
                return float(x) if x is not None and np.isfinite(x) else None
            except Exception:
                return None

        def _compute_daily_stats(df: pd.DataFrame, ts_col: str = "timestamp") -> tuple[int | None, float | None]:
            """
            Calcule (n_unique_days, std_daily) √† partir d'un DataFrame de trades/logs.
            Essaie d'abord 'pnl'/'pnl_quote'/'pnl_usd'/'daily_pnl' ; fallback sur 0 si non trouv√©s.
            """
            if df is None or df.empty or ts_col not in df.columns:
                return None, None

            # Colonnes candidates pour un PnL au niveau trade/ligne
            pnl_cols = [c for c in ["daily_pnl", "pnl", "pnl_quote", "pnl_usd", "pnl_net"] if c in df.columns]
            if not pnl_cols:
                return None, None

            try:
                ts = pd.to_datetime(df[ts_col], unit="ms", errors="coerce")  # tes timestamps sont en ms
            except Exception:
                ts = pd.to_datetime(df[ts_col], errors="coerce")

            if ts.isna().all():
                return None, None

            df_tmp = df.copy()
            df_tmp["_date"] = ts.dt.date

            # somme PnL par jour (√©vite double comptage si plusieurs trades/jour)
            daily = df_tmp.groupby("_date")[pnl_cols].sum(min_count=1)
            # si plusieurs colonnes PnL existent, somme par ligne puis std
            daily_sum = daily.sum(axis=1)

            # nombre de jours avec une observation non nulle / non NaN
            n_days = int(daily_sum.dropna().shape[0])
            std_daily = float(daily_sum.std(ddof=1)) if n_days >= 2 else 0.0

            return n_days, std_daily

        # Compl√©ter n_unique_days / std_daily si absents dans kpi
        if kpi.get("n_unique_days") is None or kpi.get("std_daily") is None:
            n_days_t, std_t = _compute_daily_stats(df_trade, ts_col="timestamp")
            n_days_l, std_l = _compute_daily_stats(df_log,   ts_col="timestamp")
            # priorit√© trades, sinon logs, sinon None
            kpi["n_unique_days"] = _safe_int(kpi.get("n_unique_days")) or n_days_t or n_days_l
            kpi["std_daily"]     = _safe_float(kpi.get("std_daily"))  or (std_t if std_t not in (None, 0) else std_l)

        # Normaliser d√©fauts
        kpi["n_unique_days"] = _safe_int(kpi.get("n_unique_days"))
        kpi["std_daily"]     = _safe_float(kpi.get("std_daily"))

        # Flags & validation
        kpi["is_valid"] = 1
        kpi["invalid_reason"] = None
        kpi["flag_sharpe_outlier"] = 0

        # R√®gle 1 : trop peu de jours actifs OU volatilit√© journali√®re trop faible
        if (kpi["n_unique_days"] is not None and kpi["n_unique_days"] < MIN_ACTIVE_DAYS) or \
        (kpi["std_daily"] is not None and kpi["std_daily"] < MIN_STD_DAILY):
            kpi["is_valid"] = 0
            kpi["invalid_reason"] = "too_few_days_or_zero_vol"

        # R√®gle 2 : Sharpe aberrant ‚Üí on **hard-reject** (recommand√©) + tag outlier
        sd365 = _safe_float(kpi.get("sharpe_d_365"))
        if sd365 is not None and np.isfinite(sd365) and abs(sd365) > SHARPE_CAP:
            kpi["flag_sharpe_outlier"] = 1
            kpi["is_valid"] = 0
            kpi["invalid_reason"] = "sharpe_outlier"
            # Optionnel: exposer un Sharpe "winsoris√©" si tu veux tracer/diagnostiquer
            kpi["sharpe_d_365_clipped"] = float(np.sign(sd365) * SHARPE_CAP)

        # UPSERT KPI avec les flags
        upsert_kpis(FINAL_DB_PATH, backtest_id, iteration, kpi)


        # ===== 5) Crit√®re de succ√®s : KPI cl√©s calcul√©s (Sharpe 365 d√©sormais) =====
        required = ["sharpe_d_365", "sortino_d_252", "max_drawdown", "profit_factor", "win_rate"]

        def _is_valid_num(x):
            try:
                return (x is not None) and not (isinstance(x, float) and (np.isnan(x) or np.isinf(x)))
            except Exception:
                return False

        has_metrics = all((m in kpi) and _is_valid_num(kpi[m]) for m in required)

        # Hard-reject : si 'is_valid' a √©t√© pos√© √† 0 (peu de jours, vol nulle, outlier), on refuse.
        if int(kpi.get("is_valid", 1)) == 0:
            reason = kpi.get("invalid_reason", "invalid")
            print(f"üö´ Backtest invalid√© (reason={reason}) ‚Äî Sharpe365={kpi.get('sharpe_d_365')} ‚Äî Params: {param_dict}")
            return False, backtest_id, kpi

        if has_metrics:
            return True, backtest_id, kpi
        else:
            missing = [m for m in required if (m not in kpi) or (not _is_valid_num(kpi[m]))]
            print(f"‚ùå KPI requis manquants/NaN: {missing} ‚Äî Params: {param_dict}")
            return False, backtest_id, kpi

    except Exception as e:
        # On essaye d'exposer backtest_id si d√©j√† g√©n√©r√©, sinon cha√Æne vide.
        try:
            bid = backtest_id
        except Exception:
            bid = ""
        print(f"[‚ùå run_backtest_from_params] Erreur: {e} | backtest_id={bid}")
        return False, bid, {}

    finally:
        # Nettoyage m√©moire apr√®s chaque run
        gc.collect()