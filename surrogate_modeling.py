"""
====================================================================================
Fichier : surrogate_modeling.py
Objectif : ModÃ©lisation interprÃ©table de la performance stratÃ©gique via surrogate model
====================================================================================

Ce module entraÃ®ne un modÃ¨le substitut (XGBoost) pour approximer une mÃ©trique cible
(Sharpe ratio) Ã  partir de l'historique des backtests (SQLite), puis produit :

  â€¢ SHAP (beeswarm & bar) + export JSON des importances globales mean(|SHAP|)
  â€¢ UMAP sur les valeurs SHAP (structure non linÃ©aire)
  
Les importances exportÃ©es (JSON) sont destinÃ©es au driver GP pour :
  â€“ construire une Trust-Region anisotrope,
  â€“ pondÃ©rer les distances de diversitÃ© intra-batch.

Auteur : Moncoucut Brandon
Version : Octobre 2025
"""

# === Imports fondamentaux ===
import os
import sys
import json
import sqlite3
from datetime import datetime

import numpy as np
import pandas as pd
import xgboost as xgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import umap
# --- Garde-fou pour la cible (cap du Sharpe annualisÃ©) ---
try:
    import config as cfg
    S_CAP = float(getattr(cfg, "SHARPE_CAP", 10.0)) #performance 5 !!!!!!!!!!
except Exception:
    S_CAP = 10.0  # dÃ©faut robuste si config indisponible #performance 5 !!!!!!!!

# === CONFIG ===
# Cible par dÃ©faut : Sharpe annualisÃ© daily standardisÃ© (table kpi_by_backtest).
# Fallback automatique sur trades.sharpe_ratio si la table KPI n'existe pas.
TARGET_METRIC = "sharpe_d_365"
EXPORT_DIR = "/Users/brandonmoncoucut/Desktop/Najas_king/Charts/surrogate_modeling"
ARTIFACTS_DIR = "/Users/brandonmoncoucut/Desktop/Najas_king/Artifacts"
MAX_ROWS = 5000  # plafond de lignes chargÃ©es (sÃ©curitÃ© mÃ©moire)

os.makedirs(EXPORT_DIR, exist_ok=True)
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

# -----------------------------------------------------------------------------
# I/O utils
# -----------------------------------------------------------------------------
def _save_plot(name: str) -> None:
    """Sauvegarde la figure matplotlib active avec un nom datÃ© (robuste)."""
    timestamp = datetime.now().strftime("%Y-%m-%d_%Hh%M")
    filename = f"{name}_{timestamp}.png"
    path = os.path.join(EXPORT_DIR, filename)
    try:
        plt.savefig(path, bbox_inches="tight")
        print(f"âœ… Figure sauvegardÃ©e : {path}")
    except Exception as e:
        print(f"âš ï¸ Sauvegarde figure Ã©chouÃ©e ({path}) : {e}")
    finally:
        plt.close()

def save_shap_importance_json(path: str, feature_names, shap_values: np.ndarray) -> None:
    """
    Sauvegarde un dict {feature: mean(|SHAP|)} en JSON pour usage GP.
    """
    try:
        mean_abs = np.mean(np.abs(shap_values), axis=0)  # shape [d]
        data = {str(name): float(val) for name, val in zip(feature_names, mean_abs)}
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w") as f:
            json.dump(data, f)
        print(f"âœ… SHAP importance JSON Ã©crit : {path}")
    except Exception as e:
        print(f"âš ï¸ Ã‰chec Ã©criture SHAP JSON ({path}) : {e}")

def _has_table(conn: sqlite3.Connection, name: str) -> bool:
    """
    Retourne True si la table SQLite `name` existe dans la base.
    """
    try:
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?;", (name,))
        return cur.fetchone() is not None
    except Exception:
        return False

# -----------------------------------------------------------------------------
# Data loading
# -----------------------------------------------------------------------------

def load_data(db_path: str, limit: int = 1000) -> pd.DataFrame:
    """
    RÃ©cupÃ¨re une ligne par backtest_id (meilleure performance) + dernier log (hyperparams).
    PrioritÃ© aux KPI standardisÃ©s (table `kpi_by_backtest`, champ TARGET_METRIC).
    Fallback automatique : `trades.sharpe_ratio` si la table KPI n'existe pas.

    Returns
    -------
    pd.DataFrame
        Colonnes : backtest_id, timestamp (UTC), hyperparams..., TARGET_METRIC
    """
    limit = int(min(limit, MAX_ROWS))
    hyperparams = [
        "timestamp", "ema_short_period", "ema_long_period", "rsi_period", "rsi_buy_zone", "rsi_sell_zone",
        "rsi_past_lookback", "atr_tp_multiplier", "atr_sl_multiplier", "atr_period",
        "macd_signal_period", "rsi_thresholds_1m", "rsi_thresholds_5m", "rsi_thresholds_15m",
        "rsi_thresholds_1h", "ewma_period", "weight_atr_combined_vol", "threshold_volume",
        "hist_volum_period", "detect_supp_resist_period", "trend_period", "threshold_factor",
        "min_profit_margin", "resistance_buffer_margin", "risk_reward_ratio", "confidence_score_params",
        "signal_weight_bonus", "penalite_resistance_factor", "penalite_multi_tf_step",
        "override_score_threshold", "rsi_extreme_threshold", "signal_pure_threshold",
        "signal_pure_weight"
    ]
    col_str = ", ".join([f"l.{c}" for c in ["backtest_id"] + hyperparams])

    conn = None
    try:
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='kpi_by_backtest';")
        has_kpi = cur.fetchone() is not None
        # Info migration : si des donnÃ©es 252 existent encore, on le signale pour Ã©viter l'ambiguÃ¯tÃ©.
        if has_kpi and TARGET_METRIC == "sharpe_d_365":
            try:
                cur.execute("SELECT 1 FROM kpi_by_backtest WHERE sharpe_d_252 IS NOT NULL LIMIT 1;")
                if cur.fetchone() is not None:
                    print("â„¹ï¸  kpi_by_backtest contient des valeurs 'sharpe_d_252' alors que TARGET_METRIC='sharpe_d_365'. "
                        "VÃ©rifie la migration/annualisation des run historiques.")
            except Exception:
                # TolÃ©rant : pas bloquant si la colonne n'existe pas
                pass

        if has_kpi:
            # --- Chemin moderne : kpi_by_backtest + best TARGET_METRIC par backtest_id
            # DÃ©tecte la prÃ©sence des colonnes de validation/outliers
            try:
                cur.execute("PRAGMA table_info(kpi_by_backtest);")
                _cols = [r[1] for r in cur.fetchall()]
            except Exception:
                _cols = []
            has_is_valid = ("is_valid" in _cols)
            has_flag_out = ("flag_sharpe_outlier" in _cols)

            # Clause de filtre SQL si colonnes prÃ©sentes
            filter_clause = ""
            if has_is_valid or has_flag_out:
                # coalesce pour gÃ©rer valeurs NULL comme Â« OK sauf si explicite Â»
                _f1 = "AND COALESCE(is_valid,1)=1" if has_is_valid else ""
                _f2 = "AND COALESCE(flag_sharpe_outlier,0)=0" if has_flag_out else ""
                filter_clause = f"{_f1} {_f2}"

                # Stats dâ€™exclusion (avant/aprÃ¨s)
                try:
                    cur.execute(f"SELECT COUNT(DISTINCT backtest_id) FROM kpi_by_backtest WHERE {TARGET_METRIC} IS NOT NULL;")
                    tot = int(cur.fetchone()[0] or 0)
                    cur.execute(f"SELECT COUNT(DISTINCT backtest_id) FROM kpi_by_backtest WHERE {TARGET_METRIC} IS NOT NULL {_f1} {_f2};")
                    ok = int(cur.fetchone()[0] or 0)
                    if tot > 0 and ok <= tot:
                        drop_pct = 100.0 * (tot - ok) / tot
                        print(f"ðŸ§¹ Filtre KPI: is_valid/!outlier â€” gardÃ©s={ok}/{tot} ({100.0 - drop_pct:.1f}% kept, {drop_pct:.1f}% dropped)")
                except Exception:
                    pass

            query = f"""
                WITH ranked AS (
                    SELECT
                        backtest_id,
                        iteration,
                        {TARGET_METRIC} AS metric,
                        ROW_NUMBER() OVER (
                            PARTITION BY backtest_id
                            ORDER BY {TARGET_METRIC} DESC
                        ) AS rn
                    FROM kpi_by_backtest
                    WHERE {TARGET_METRIC} IS NOT NULL
                    {filter_clause}
                ),
                top AS (
                    SELECT backtest_id, iteration, metric
                    FROM ranked
                    WHERE rn = 1
                    ORDER BY metric DESC
                    LIMIT {limit}
                ),
                mx AS (
                    SELECT l.backtest_id, MAX(l.timestamp) AS ts
                    FROM logs l
                    JOIN top t ON l.backtest_id = t.backtest_id
                    GROUP BY l.backtest_id
                )
                SELECT {col_str}, t.metric AS {TARGET_METRIC}, t.iteration
                FROM logs l
                JOIN mx  ON l.backtest_id = mx.backtest_id AND l.timestamp = mx.ts
                JOIN top t ON l.backtest_id = t.backtest_id
            """
            df = pd.read_sql_query(query, conn)
            # Si la table KPI existe mais ne retourne rien (toutes valeurs NULL), on fera fallback ci-dessous.
            if df.empty:
                has_kpi = False

        if not has_kpi:
            # --- Fallback hÃ©ritÃ© : best trades.sharpe_ratio (par backtest_id + iteration)
            query = f"""
                WITH ranked AS (
                    SELECT
                        backtest_id,
                        iteration,
                        sharpe_ratio AS metric,
                        ROW_NUMBER() OVER (
                            PARTITION BY backtest_id
                            ORDER BY sharpe_ratio DESC
                        ) AS rn
                    FROM trades
                    WHERE sharpe_ratio IS NOT NULL
                ),
                top AS (
                    SELECT backtest_id, iteration, metric
                    FROM ranked
                    WHERE rn = 1
                    ORDER BY metric DESC
                    LIMIT {limit}
                ),
                mx AS (
                    SELECT l.backtest_id, MAX(l.timestamp) AS ts
                    FROM logs l
                    JOIN top t ON l.backtest_id = t.backtest_id
                    GROUP BY l.backtest_id
                )
                SELECT {col_str}, t.metric AS sharpe_ratio, t.iteration
                FROM logs l
                JOIN mx  ON l.backtest_id = mx.backtest_id AND l.timestamp = mx.ts
                JOIN top t ON l.backtest_id = t.backtest_id
            """
            df = pd.read_sql_query(query, conn)
    finally:
        if conn is not None:
            conn.close()

    # Harmonisation UTC
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)

    # Harmonisation de la colonne cible (assure TARGET_METRIC dans tous les cas)
    if TARGET_METRIC not in df.columns:
        # Fallback hÃ©ritÃ© : mappe depuis trades.sharpe_ratio si prÃ©sent.
        # âš ï¸ Attention : l'Ã©chelle peut diffÃ©rer de l'annualisation 365 ; c'est un repli, pas la voie nominale.
        if "sharpe_ratio" in df.columns:
            print(f"âš ï¸  Fallback: utilisation de 'trades.sharpe_ratio' pour alimenter '{TARGET_METRIC}'. "
                "VÃ©rifie l'unitÃ©/annualisation.")
            df[TARGET_METRIC] = df["sharpe_ratio"]
        else:
            raise ValueError(f"âŒ La colonne cible '{TARGET_METRIC}' est absente des rÃ©sultats.")

    # Nettoyage numÃ©rique (Ã©vite NaN/Inf cÃ´tÃ© modeling)
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    df[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)

    return df

# -----------------------------------------------------------------------------
# Modeling
# -----------------------------------------------------------------------------

def print_dataset_diagnostics(
    X: pd.DataFrame,
    y: pd.Series,
    *,
    y_name: str = TARGET_METRIC,
    alert_std_eps: float = 1e-8,
    alert_unique_k: int = 3,
) -> None:
    """
    Affiche des diagnostics "qualitÃ© signal" pour (X, y).

    ParamÃ¨tres
    ----------
    X : pd.DataFrame
        Matrice d'hyperparamÃ¨tres (features numÃ©riques).
    y : pd.Series
        Cible (par dÃ©faut, Sharpe annualisÃ© `TARGET_METRIC`).
    y_name : str
        Nom lisible de la cible pour les logs.
    alert_std_eps : float
        Seuil sous lequel l'Ã©cart-type est considÃ©rÃ© comme quasi nul.
    alert_unique_k : int
        Seuil sous lequel le nombre de valeurs uniques de y est jugÃ© trop faible.

    Logs produits
    -------------
    â€¢ Taille et taux de NaN de X et y
    â€¢ Nombre de valeurs uniques de y + stats (moyenne / Ã©cart-type / quantiles)
    â€¢ Variance par feature (triÃ©e, pour repÃ©rer les colonnes quasi constantes)
    â€¢ Alertes si std(y) ~ 0 ou si y a trÃ¨s peu de valeurs uniques
    """
    # --- Taille / NaN ---
    n, d = X.shape
    x_nan_ratio = X.isna().mean().mean() if n * d > 0 else float("nan")
    y_nan_ratio = float(y.isna().mean()) if len(y) > 0 else float("nan")
    print(f"ðŸ§ª DIAG: X shape={X.shape}, NaN_rateâ‰ˆ{x_nan_ratio:.4f} | y len={len(y)}, NaN_rateâ‰ˆ{y_nan_ratio:.4f}")

    # --- y : uniques + stats ---
    y_clean = y.replace([np.inf, -np.inf], np.nan).dropna()
    nunique_y = int(y_clean.nunique())
    y_mean = float(y_clean.mean()) if len(y_clean) else float("nan")
    y_std  = float(y_clean.std(ddof=1)) if len(y_clean) > 1 else 0.0
    q = y_clean.quantile([0.05, 0.25, 0.50, 0.75, 0.95]) if len(y_clean) else pd.Series(dtype=float)

    print(f"ðŸ§ª DIAG: y='{y_name}' | uniques={nunique_y} | mean={y_mean:.6f} | std={y_std:.6f}")
    if not q.empty:
        print("ðŸ§ª DIAG: y quantiles (5/25/50/75/95%): " +
              ", ".join([f"{int(p*100)}%={q.loc[p]:.6f}" for p in [0.05, 0.25, 0.50, 0.75, 0.95]]))

    # --- Variance par feature (pour colonnes quasi constantes) ---
    # On remplit temporairement les NaN par la mÃ©diane pour ne pas biaiser la variance.
    X_num = X.copy()
    for c in X_num.columns:
        if X_num[c].isna().any():
            med = X_num[c].median(skipna=True)
            X_num[c] = X_num[c].fillna(med)
    variances = X_num.var(ddof=0).sort_values()  # ddof=0 -> variance population
    low_var = variances.head(min(10, len(variances)))
    print("ðŸ§ª DIAG: plus faibles variances (top 10) â†’")
    for feat, v in low_var.items():
        print(f"   â€¢ {feat}: var={v:.6e}")

    # --- Alertes ---
    if y_std <= alert_std_eps:
        print(f"ðŸš¨ ALERTE: std(y)â‰ˆ0 (std={y_std:.3e}) â†’ cible quasi constante / mal construite.")
    if nunique_y < alert_unique_k:
        print(f"ðŸš¨ ALERTE: y a trÃ¨s peu de valeurs uniques (nunique={nunique_y}) â†’ cible quasi constante / mal construite.")

def train_xgb(df: pd.DataFrame):
    """
    EntraÃ®ne un XGBoost rÃ©gularisÃ© pour approximer TARGET_METRIC Ã  partir des hyperparams.
    Retourne : (model, X, y)
    """
    exclude = ["timestamp", "backtest_id", "iteration", TARGET_METRIC]
    param_cols = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]

    X = df[param_cols].replace([np.inf, -np.inf], np.nan).dropna()
    y_raw = df.loc[X.index, TARGET_METRIC].astype(float)

    # Cap de sÃ©curitÃ© (anti-valeurs aberrantes)
    y = np.clip(y_raw, -S_CAP, S_CAP)
    try:
        clip_ratio = float((np.abs(y_raw.values) > S_CAP).mean())
        if clip_ratio > 0.0:
            print(f"ðŸ›¡ï¸  Target clip: |y|>S_CAP sur {clip_ratio*100:.1f}% des points (S_CAP={S_CAP})")
    except Exception:
        pass

    # ModÃ¨le un peu rÃ©gularisÃ© + seed fixe (stabilitÃ© et gÃ©nÃ©ralisation)
    model = xgb.XGBRegressor(
        n_estimators=400,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_lambda=2.0,
        random_state=42,
        n_jobs=0
    )
    model.fit(X, y)
    return model, X, y

# -----------------------------------------------------------------------------
# SHAP & viz
# -----------------------------------------------------------------------------
def compute_shap(model, X: pd.DataFrame):
    """
    Calcule les valeurs SHAP (numpy) + tracÃ©s globaux.
    Retourne (shap_values: np.ndarray).
    """
    # Explainer & values
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)  # np.ndarray [n, d]

    # Expected value compat (scalaire ou vecteur)
    try:
        base_vals = explainer.expected_value
        if isinstance(base_vals, (list, tuple, np.ndarray)):
            base_vals = np.array(base_vals).ravel()
            base_vals = float(np.mean(base_vals))
    except Exception:
        base_vals = 0.0

    # Beeswarm
    try:
        shap_obj = shap.Explanation(
            values=shap_values,
            base_values=np.full((shap_values.shape[0],), base_vals, dtype=float),
            data=X.values,
            feature_names=X.columns
        )
        shap.plots.beeswarm(shap_obj, max_display=20, show=False)
        plt.title(f"SHAP beeswarm â€” {TARGET_METRIC} (capÃ© modÃ¨le Â±{S_CAP})")
        _save_plot(f"shap_beeswarm_{TARGET_METRIC}")

    except Exception as e:
        print(f"âš ï¸ Beeswarm non gÃ©nÃ©rÃ© : {e}")

    # Bar
    try:
        shap.plots.bar(shap_obj, max_display=20, show=False)
        plt.title(f"SHAP importance â€” {TARGET_METRIC} (capÃ© modÃ¨le Â±{S_CAP})")
        _save_plot(f"shap_bar_{TARGET_METRIC}")

    except Exception as e:
        print(f"âš ï¸ Bar plot non gÃ©nÃ©rÃ© : {e}")

    return shap_values

def save_pdp_2d(*args, **kwargs):
    """
    PDP dÃ©sactivÃ© : no-op (gardÃ© pour compat ascendante).
    """
    return

def umap_on_shap(shap_values: np.ndarray, y: pd.Series):
    """
    UMAP des SHAP values (aprÃ¨s standardisation) colorÃ© par y (Sharpe).
    """
    try:
        scaler = StandardScaler()
        shap_scaled = scaler.fit_transform(shap_values)

        reducer = umap.UMAP(n_components=2, random_state=42)
        emb = reducer.fit_transform(shap_scaled)

        df_emb = pd.DataFrame(emb, columns=["UMAP1", "UMAP2"])
        df_emb["Sharpe"] = y.values

        sns.scatterplot(data=df_emb, x="UMAP1", y="UMAP2", hue="Sharpe", palette="coolwarm")
        plt.title(f"UMAP des SHAP â€” {TARGET_METRIC} (capÃ© modÃ¨le Â±{S_CAP})")
        _save_plot(f"shap_umap_{TARGET_METRIC}")

    except Exception as e:
        print(f"âš ï¸ UMAP non gÃ©nÃ©rÃ© : {e}")

# -----------------------------------------------------------------------------
# Pipeline
# -----------------------------------------------------------------------------
def run_surrogate_pipeline(db_path: str, top_k: int = 1000):
    """
    1) Chargement (top_k backtests uniques, meilleurs Sharpe / id)
    2) EntraÃ®nement XGB (SHARPE ~ hyperparams)
    3) SHAP + export JSON (Artifacts/shap_importance.json)
    4) UMAP sur SHAP
    """
    print("ðŸ“¥ Chargement...")
    print(f"ðŸŽ¯ Target = {TARGET_METRIC} (capÃ© modÃ¨le Â±{S_CAP}) â€” filtres data: is_valid=1 & !outlier (pas de filtre cap).")
    df = load_data(db_path, limit=top_k)

    print("ðŸ§  EntraÃ®nement modÃ¨le XGBoost...")
    model, X, y = train_xgb(df)

    # --- Export de la liste des features (pour le GP) ---
    try:
        feature_list = list(map(str, X.columns))
        feats_path = os.path.join(ARTIFACTS_DIR, "surrogate_features.json")
        with open(feats_path, "w") as f:
            json.dump({"feature_list": feature_list}, f)
        print(f"âœ… Feature list exportÃ©e : {feats_path} ({len(feature_list)} features)")
    except Exception as e:
        print(f"âš ï¸ Impossible d'Ã©crire surrogate_features.json : {e}")

    # ðŸ§ª Diagnostics qualitÃ© signal (X/y)
    print("ðŸ§ª Diagnostics X/y (qualitÃ© signal)...")
    print_dataset_diagnostics(X, y, y_name=TARGET_METRIC)

    # VÃ©rifier que y nâ€™est pas dÃ©gÃ©nÃ©rÃ©e (trop peu de valeurs / variance quasi nulle)
    y_clean = pd.Series(y).replace([np.inf, -np.inf], np.nan).dropna()
    y_std = float(y_clean.std(ddof=1)) if len(y_clean) > 1 else 0.0
    y_nuniq = int(y_clean.nunique())

    if (y_std <= 1e-12) or (y_nuniq < 3):
        print(f"ðŸš« SHAP sautÃ©: y dÃ©gÃ©nÃ©rÃ©e (std={y_std:.3e}, nunique={y_nuniq}).")
        shap_values = None
    else:
        print("ðŸ” Analyse SHAP...")
        shap_values = compute_shap(model, X)

        # Export importances globales (consommÃ©es par gp_driver.py)
        shap_json_path = os.path.join(ARTIFACTS_DIR, "shap_importance.json")
        save_shap_importance_json(shap_json_path, X.columns, shap_values)

        print("ðŸŒŒ UMAP sur SHAP values...")
        umap_on_shap(shap_values, y)

    print("\nðŸŽ¯ Analyse terminÃ©e. RÃ©sultats dans :", EXPORT_DIR)

# -----------------------------------------------------------------------------
# EntrÃ©e CLI
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("âŒ Usage: python surrogate_modeling.py <db_path>")
        sys.exit(1)
    DB_PATH = sys.argv[1]
    run_surrogate_pipeline(DB_PATH)
